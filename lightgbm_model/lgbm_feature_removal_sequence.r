library(dplyr)
library(caret)
library(lightgbm)
library(readr)

run_removal_effect_on_test <- function(
  train_df,
  test_df,
  importance_df,
  best_params,
  categorical_cols,
  k_folds   = 5,
  max_remove= 10
) {
  print(match.call())
  # 1) æŠŠç‰¹å¾æŒ‰ Gain ä»å°åˆ°å¤§æ’å¥½
  ranked_feats <- importance_df %>%
    arrange(Gain) %>%
    pull(Feature)

  # 2) å…ˆåœ¨ train_df ä¸Šåšä¸€æ¬¡åˆ†ç®± stratifyï¼Œï¼ˆåé¢ CV ç”¨ï¼‰
  train_df$bin <- cut(
    train_df$ç¸½é¡å…ƒ,
    breaks = quantile(train_df$ç¸½é¡å…ƒ, probs = seq(0,1, length.out = k_folds+1)),
    include.lowest = TRUE
  )
  folds <- createFolds(train_df$bin, k = k_folds, returnTrain = TRUE)
  train_df$bin <- NULL

  # 4) ç»“æœå®¹å™¨
  res <- tibble(
    Num_Removed      = integer(),
    Removed_Features = character(),
    Train_RMSE       = numeric(),
    Test_RMSE        = numeric(),
    Test_MAPE        = numeric(),
    Test_MEAPE       = numeric(),
    Test_SDPE        = numeric()
  )

  set.seed(42)
  for (n in 0:max_remove) {
    print(n)
    # 5) å–å‡ºè¦å‰”é™¤çš„ n ä¸ªæœ€å° Gain features
    low_feats <- if (n==0) character(0) else ranked_feats[1:n]
    use_feats <- setdiff(names(train_df), c("ç¸½é¡å…ƒ", low_feats))
    if (length(use_feats)==0) next

    # â€”â€”â€” 5.1 CV è®¡ç®—å¹³å‡ Train RMSE â€”â€”â€”
    rmse_cv <- c()
    for (f in seq_along(folds)) {
      tr <- train_df[ folds[[f]],   ]
      va <- train_df[-folds[[f]],   ]
      mtr <- data.matrix(tr[ , use_feats])
      mva <- data.matrix(va[ , use_feats])
      cat_feats <- intersect(categorical_cols, use_feats)

      dtr <- lgb.Dataset(
        data = mtr,
        label= log(tr$ç¸½é¡å…ƒ),
        categorical_feature = cat_feats
      )
      mdl <- lgb.train(
        params   = c(best_params, list(objective="regression", metric="rmse")),
        data     = dtr,
        nrounds  = best_params$nrounds,
        verbose  = -1
      )
      preds_va <- exp(predict(mdl, mva))
      rmse_cv   <- c(rmse_cv, sqrt(mean((preds_va - va$ç¸½é¡å…ƒ)^2)))
    }
    train_rmse_mean <- mean(rmse_cv)

    # â€”â€”â€” 5.2 ç”¨å…¨éƒ¨ train_df è®­ç»ƒ finalï¼Œå†è·‘ test_df â€”â€”â€”
    full_mat <- data.matrix(train_df[ , use_feats])
    dfull    <- lgb.Dataset(
      data = full_mat,
      label= log(train_df$ç¸½é¡å…ƒ),
      categorical_feature = intersect(categorical_cols, use_feats)
    )
    mdl_full  <- lgb.train(
      params   = c(best_params, list(objective="regression", metric="rmse")),
      data     = dfull,
      nrounds  = best_params$nrounds,
      verbose  = -1
    )
    test_mat  <- data.matrix(test_df[ , use_feats])
    preds_te  <- exp(predict(mdl_full, test_mat))
    test_rmse <- sqrt(mean((preds_te - test_df$ç¸½é¡å…ƒ)^2))
    test_mape <- mean(abs(preds_te - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)
    test_meape<- median(abs(preds_te - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)
    test_sdpe <- sd(abs(preds_te - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)

    # 6) å­˜ç»“æœ
    res <- add_row(
      res,
      Num_Removed      = n,
      Removed_Features = paste(low_feats, collapse = ", "),
      Train_RMSE       = train_rmse_mean,
      Test_RMSE        = test_rmse,
      Test_MAPE        = test_mape,
      Test_MEAPE       = test_meape,
      Test_SDPE        = test_sdpe
    )
  }

  return(res)
}

# æŒ‡å®šè¦è®€å–çš„æ¬„ä½
use_columns <- c(
  "é„‰é®å¸‚å€", "ç¸½é¡å…ƒ", "ç§Ÿè³ƒå¹´æœˆæ—¥",
  "å‡ºç§Ÿå‹æ…‹",  
  "ç§Ÿè³ƒå±¤æ¬¡(å››é¡)", "ç¸½æ¨“å±¤æ•¸", "å»ºç‰©å‹æ…‹", 
  "äº¤æ˜“ç­†æ£Ÿæ•¸-åœŸåœ°", "äº¤æ˜“ç­†æ£Ÿæ•¸-å»ºç‰©",
  "ç§Ÿè³ƒä½å®…æœå‹™", "ç§Ÿè³ƒå¤©æ•¸",
  "æœ‰ç„¡ç®¡ç†çµ„ç¹”", "æœ‰ç„¡ç®¡ç†å“¡", "æœ‰ç„¡é™„å‚¢ä¿±","æœ‰ç„¡é›»æ¢¯", 
  "å»ºç‰©ç¾æ³æ ¼å±€-æˆ¿", "å»ºç‰©ç¾æ³æ ¼å±€-å»³","å»ºç‰©ç¾æ³æ ¼å±€-è¡›", "å»ºç‰©ç¾æ³æ ¼å±€-éš”é–“", 
  "å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º", 
  "å±‹é½¡",
  "å»ºæåˆ†é¡",
  "é™„å±¬è¨­å‚™-å†·æ°£", "é™„å±¬è¨­å‚™-ç†±æ°´å™¨", "é™„å±¬è¨­å‚™-æ´—è¡£æ©Ÿ","é™„å±¬è¨­å‚™-é›»è¦–æ©Ÿ", "é™„å±¬è¨­å‚™-å†°ç®±", "é™„å±¬è¨­å‚™-ç“¦æ–¯æˆ–å¤©ç„¶æ°£", "é™„å±¬è¨­å‚™-æœ‰ç·šé›»è¦–", "é™„å±¬è¨­å‚™-ç¶²è·¯",
  "æ·é‹ç«™è·é›¢(å…¬å°º)",
  "æ–‡æ¹–ç·š", "æ·¡æ°´ä¿¡ç¾©ç·š", "æ–°åŒ—æŠ•æ”¯ç·š", "æ¾å±±æ–°åº—ç·š", "å°ç¢§æ½­æ”¯ç·š", 
  "ä¸­å’Œæ–°è˜†ç·š", "æ¿å—ç·š", "ç’°ç‹€ç·š", "é™„è¿‘å»ºç‰©å–®ä½æˆäº¤å‡åƒ¹"
) 

# è®€å–è³‡æ–™
full_data <- read_csv("rent_mrg.csv", show_col_types = FALSE)
# ç¯©é¸æ¬„ä½
df <- full_data %>% select(all_of(use_columns))

## ç¯©æ‰ã€Œå¤©æ•¸å¤ªçŸ­ã€çš„è³‡æ–™ 
df <- df %>% filter(ç§Ÿè³ƒå¤©æ•¸ >= 30)

# å‡è¨­ä½ åŸå§‹è³‡æ–™å« df
df <- df %>%
  mutate(æ·é‹ç·š = case_when(
    æ–‡æ¹–ç·š == 1 ~ "æ–‡æ¹–ç·š",
    æ·¡æ°´ä¿¡ç¾©ç·š == 1 ~ "æ·¡æ°´ä¿¡ç¾©ç·š",
    æ–°åŒ—æŠ•æ”¯ç·š == 1 ~ "æ–°åŒ—æŠ•æ”¯ç·š",
    æ¾å±±æ–°åº—ç·š == 1 ~ "æ¾å±±æ–°åº—ç·š",
    å°ç¢§æ½­æ”¯ç·š == 1 ~ "å°ç¢§æ½­æ”¯ç·š",
    ä¸­å’Œæ–°è˜†ç·š == 1 ~ "ä¸­å’Œæ–°è˜†ç·š",
    æ¿å—ç·š == 1 ~ "æ¿å—ç·š",
    ç’°ç‹€ç·š == 1 ~ "ç’°ç‹€ç·š",
    TRUE ~ "ç„¡æ·é‹"
  )) %>%
  select(-c(æ–‡æ¹–ç·š, æ·¡æ°´ä¿¡ç¾©ç·š, æ–°åŒ—æŠ•æ”¯ç·š, æ¾å±±æ–°åº—ç·š, å°ç¢§æ½­æ”¯ç·š, 
            ä¸­å’Œæ–°è˜†ç·š, æ¿å—ç·š, ç’°ç‹€ç·š)) # ç§»é™¤åŸ one-hot æ¬„ä½

df$å‡ºç§Ÿå‹æ…‹[is.na(df$å‡ºç§Ÿå‹æ…‹)] <- "æœªçŸ¥"
df$ç§Ÿè³ƒä½å®…æœå‹™[is.na(df$ç§Ÿè³ƒä½å®…æœå‹™)] <- "æœªçŸ¥"
df$ç§Ÿè³ƒå¹´æœˆæ—¥ <- as.numeric(df$ç§Ÿè³ƒå¹´æœˆæ—¥)


# æŒ‡å®šåˆ†é¡æ¬„ä½ ä¸¦ç¯©æ‰æ²’ç”¨çš„features
categorical_cols <- c("é„‰é®å¸‚å€", "å‡ºç§Ÿå‹æ…‹", "ç§Ÿè³ƒå±¤æ¬¡(å››é¡)", "å»ºç‰©å‹æ…‹", "ç§Ÿè³ƒä½å®…æœå‹™", "æ·é‹ç·š", "å»ºæåˆ†é¡")
# å°‡åˆ†é¡æ¬„ä½è½‰ç‚ºå› å­é¡å‹
df[categorical_cols] <- lapply(df[categorical_cols], factor)


# 
set.seed(42)
df$price_bin <- cut(df$ç¸½é¡å…ƒ,
                    breaks = quantile(df$ç¸½é¡å…ƒ, probs = seq(0, 1, 0.2), na.rm = TRUE),
                    include.lowest = TRUE)
train_idx <- createDataPartition(df$price_bin, p = 0.8, list = FALSE)
train_df <- df[train_idx, ] %>% select(-price_bin)
test_df  <- df[-train_idx, ] %>% select(-price_bin)


# 3ï¸âƒ£ åœ¨ train_df ä¸Šåš K-fold CVï¼ˆtune è¶…åƒæ•¸ï¼‰
k <- 10
train_df$price_bin <- cut(train_df$ç¸½é¡å…ƒ,
                          breaks = quantile(train_df$ç¸½é¡å…ƒ, probs = seq(0, 1, length.out = k + 1), na.rm = TRUE),
                          include.lowest = TRUE)
folds <- createFolds(train_df$price_bin, k = k, returnTrain = TRUE)
train_df <- train_df %>% select(-c(price_bin))

grid <- expand.grid(
  num_leaves = c(31),  # å¯æ ¹æ“šéœ€è¦èª¿æ•´
  learning_rate = c(0.05),
  feature_fraction = c(0.8),
  min_data_in_leaf = c(20),  # å¯æ ¹æ“šéœ€è¦èª¿æ•´
  nrounds = c(500)  # å¯æ ¹æ“šéœ€è¦èª¿æ•´
)

best_rmse <- Inf
best_params <- list()
results_k_fold_tv <- data.frame()
print(colnames(train_df))

for (i in 1:nrow(grid)) {
  params <- as.list(grid[i, ])
  rmse_list <- c()
  mape_list <- c()
  medape_list <- c()
  sdpe_list <- c()

  for (f in 1:length(folds)) {
    train_data <- train_df[folds[[f]], ]
    valid_data <- train_df[-folds[[f]], ]


    # categorical_cols ç‚ºåŸå§‹ factor æ¬„ä½åç¨±
    # å°‡åˆ†é¡æ¬„ä½è½‰ç‚ºæ•¸å­—ç·¨ç¢¼
    mat <- data.matrix(train_data %>% select(-ç¸½é¡å…ƒ))
    colnames(mat) <- setdiff(names(train_data), "ç¸½é¡å…ƒ")
    
    valid_mat <- data.matrix(valid_data %>% select(-ç¸½é¡å…ƒ))
    colnames(valid_mat) <- setdiff(names(valid_data), "ç¸½é¡å…ƒ")

    # è¨“ç·´è³‡æ–™ç”¨ log(price)
    dtrain <- lgb.Dataset(data = mat,
                          label = log(train_data$ç¸½é¡å…ƒ),
                          categorical_feature = categorical_cols)
    # é©—è­‰è³‡æ–™ label å¯çœç•¥ï¼Œå› ç‚ºæˆ‘å€‘è‡ªå·±è¨ˆç®—èª¤å·®
    dvalid <- lgb.Dataset(data = valid_mat,
                          label = log(valid_data$ç¸½é¡å…ƒ),
                          categorical_feature = categorical_cols)

    model <- lgb.train(
      params = c(params, list(objective = "regression", metric = "rmse")),
      data = dtrain,
      nrounds = params$nrounds,
      valids = list(valid = dvalid),
      verbose = -1,
      #early_stopping_rounds = 100
    )

     # é æ¸¬çš„æ˜¯ log(price)ï¼Œè¦è½‰å›åŸæœ¬å–®ä½
    preds_logs <- predict(model, valid_mat)
    preds <- exp(preds_logs) # é‚„åŸç‚ºç§Ÿé‡‘é‡‘é¡ï¼ˆå…ƒï¼‰

    # è¨ˆç®—èª¤å·®ï¼ˆè·ŸåŸå§‹ç§Ÿé‡‘æ¯”ï¼‰
    rmse <- sqrt(mean((preds - valid_data$ç¸½é¡å…ƒ)^2))
    mape <- mean(abs(preds - valid_data$ç¸½é¡å…ƒ) / valid_data$ç¸½é¡å…ƒ)
    medape <- median(abs(preds - valid_data$ç¸½é¡å…ƒ) / valid_data$ç¸½é¡å…ƒ)
    sdpe <- sd(abs(preds - valid_data$ç¸½é¡å…ƒ) / valid_data$ç¸½é¡å…ƒ)

    # è¨ˆç®—èª¤å·®ï¼ˆè·ŸåŸå§‹ç§Ÿé‡‘æ¯”ï¼‰
    rmse_list <- c(rmse_list, rmse)
    mape_list <- c(mape_list, mape)
    medape_list <- c(medape_list, medape)
    sdpe_list <- c(sdpe_list, sdpe)
  }

  avg_rmse <- mean(rmse_list)
  avg_mape <- mean(mape_list)
  avg_medape <- mean(medape_list)
  avg_sdpe <- mean(sdpe_list)

  cat(sprintf("Grid %d â†’ CV RMSE: %.2f | MAPE: %.4f | MEAPE: %4f | SDPE: %4f\n", i, avg_rmse, avg_mape, avg_medape, avg_sdpe))

  if (avg_rmse < best_rmse) {
    best_rmse <- avg_rmse
    best_params <- params
  }
}

cat("Best Parameters:\n")
print(best_params)

train_mat <- data.matrix(train_df %>% select(-ç¸½é¡å…ƒ))
colnames(train_mat) <- setdiff(names(train_df), "ç¸½é¡å…ƒ")

# 4.æœ€çµ‚æ¨¡å‹ï¼šåœ¨å®Œæ•´ train_df ä¸Šè¨“ç·´ï¼Œtest_df ä¸Šé æ¸¬
final_model <- lgb.train(
  params = c(best_params, list(objective = "regression", metric = "rmse")),
  data = lgb.Dataset(data = train_mat, 
                    label = log(train_df$ç¸½é¡å…ƒ),
                    categorical_feature = categorical_cols),
  nrounds = best_params$nrounds,
  verbose = -1
)


# é æ¸¬ test_df
test_mat <- data.matrix(test_df %>% select(-ç¸½é¡å…ƒ))
colnames(test_mat) <- setdiff(names(test_df), "ç¸½é¡å…ƒ")
test_preds_log <- predict(final_model, test_mat)
test_preds <- exp(test_preds_log)
test_rmse <- sqrt(mean((test_preds - test_df$ç¸½é¡å…ƒ)^2))
test_mape <- mean(abs(test_preds - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)
test_meape <- median(abs(test_preds - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)
test_sdpe <- sd(abs(test_preds - test_df$ç¸½é¡å…ƒ) / test_df$ç¸½é¡å…ƒ)

cat("\nğŸ¯ Final Test RMSE:", round(test_rmse, 2), "\n")
cat("ğŸ¯ Final Test MAPE:", round(test_mape, 4), "\n")
cat("ğŸ¯ Final Test MEAPE:", round(test_meape, 4), "\n")
cat("ğŸ¯ Final Test SDPE:", round(test_sdpe, 4), "\n")

# è¼¸å‡ºé æ¸¬çµæœ
final_result <- test_df %>%
  mutate(Predicted = test_preds,
         AbsError = abs(Predicted - ç¸½é¡å…ƒ),
         MAPE = abs(Predicted - ç¸½é¡å…ƒ) / ç¸½é¡å…ƒ)
write.csv(final_result, "test_prediction_results.csv", row.names = FALSE)  # å¯ç”¨æ–¼å¾ŒçºŒåˆ†æ

# importance æ˜¯ä½ å‰é¢ final model è¨“ç·´å¾Œå¾—åˆ°çš„
importance <- lgb.importance(final_model, percentage = TRUE)

importance$Gain <- format(importance$Gain, scientific = FALSE, digits = 5)
importance$Cover <- format(importance$Cover, scientific = FALSE, digits = 5)
importance$Frequency <- format(importance$Frequency, scientific = FALSE, digits = 5)
print(importance)


print(best_params)
str(best_params)
res <- results <- run_removal_effect_on_test(
  train_df     = train_df,
  test_df      = test_df,
  importance_df = importance,
  best_params  = best_params,
  categorical_cols = c(
    "é„‰é®å¸‚å€", "å‡ºç§Ÿå‹æ…‹", "ç§Ÿè³ƒå±¤æ¬¡(å››é¡)", "å»ºç‰©å‹æ…‹", "ç§Ÿè³ƒä½å®…æœå‹™", "æ·é‹ç·š", "å»ºæåˆ†é¡"),  # é€™æ˜¯åˆ†é¡æ¬„ä½
  k_folds     = 10,              # ä½ æƒ³ç”¨å¹¾æŠ˜äº¤å‰é©—è­‰
  max_remove   = 20             # ä½ æƒ³è©¦å‰ 1~20 æ”¯ç‰¹å¾µ
)
write.csv(res, "feature_removal_results.csv", row.names=FALSE, fileEncoding="UTF-8")